{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "azZFyENLZm_g",
        "DeKRcpe49VnV",
        "jY-QAVIhcDDR",
        "r843-s6ZcF8C"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azZFyENLZm_g"
      },
      "source": [
        "# Introduktion til NumPy\n",
        "\n",
        "[Numpy](https://numpy.org/doc/stable/index.html) er et Python-bibliotek til oprettelse og manipulation af matricer, den primære datastruktur, der bruges af maskinlæringsalgoritmer. [Matricer](https://da.wikipedia.org/wiki/Matrix_(matematik)) er matematiske objekter, der bruges til at gemme værdier i rækker og kolonner. I kender dem nok fra mange andre sprog og strukturen i en SQL database. Derfor er det også nemt at få data fra en central SQL-database og bruge det til machinelearning.\n",
        "\n",
        "I Python kaldes matricer *lists*, NumPy kalder dem *arrays*, og TensorFlow kalder dem *tensors*. Python repræsenterer matricer med datatypen [liste](https://docs.python.org/3/library/stdtypes.html#lists).\n",
        "\n",
        "NumPy er et stort bibliotek som bruges i flere grene, vi kommer ikke til at gå i dybden med NumPy, men vi skal have en grundlæggende forståelse for at kunne bruge det til resten af vores 10 dages ML kursus.\n",
        "\n",
        "Mange af operationerne kender I nok fra andre sprog, men så har I også muligheden for at vende jer lidt til Python syntaxen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9RWewwFwX6"
      },
      "source": [
        "## Import NumPy module\n",
        "\n",
        "Vi skal installere NumPy modulet for at kunne bruge det senere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "guvPzSWYJGZ4"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cT9fXS_JUpa"
      },
      "source": [
        "## Populate arrays with specific numbers\n",
        "\n",
        "Skriv `np.array` for at lave en NumPy array, med værdi som du selv vælger. Her er et eksempel med `np.array` som laver en 8-elementer langt en dimensionelt array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XxJR5xKpJbB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1239c88e-beca-4cd2-e0d8-6e0b5a93d7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2 2.4 3.5 4.7 6.1 7.2 8.3 9.5]\n"
          ]
        }
      ],
      "source": [
        "one_dimensional_array = np.array([1.2, 2.4, 3.5, 4.7, 6.1, 7.2, 8.3, 9.5])\n",
        "print(one_dimensional_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKywqhLTbR1M"
      },
      "source": [
        "Du kan også bruge `np.array` til at oprette en todimensionel matrix. For at oprette en todimensionel matrix skal du angive en ekstra lag af firkantede parenteser. For eksempel opretter følgende kald en 3x2 matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_veGj18eMCDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1870f86-5caa-491d-d846-5dd67e5de69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6  5]\n",
            " [11  7]\n",
            " [ 4  8]]\n"
          ]
        }
      ],
      "source": [
        "two_dimensional_array = np.array([[6, 5], [11, 7], [4, 8]])\n",
        "print(two_dimensional_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ED7eug9CvGR"
      },
      "source": [
        "For at udfylde en matrix rene nuller, kald `np.zeros`. For at udfylde en matrix med rene ettere, kald `np.ones`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEy_pdBoROu3"
      },
      "source": [
        "## Udfyld arrays med sekvenser af tal\n",
        "\n",
        "Du kan udfylde et array med en sekvens af tal:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CjHfYWhdQYtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0792007-df8c-404b-a6d2-926540a23cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5  6  7  8  9 10 11]\n"
          ]
        }
      ],
      "source": [
        "sequence_of_integers = np.arange(5, 12)\n",
        "print(sequence_of_integers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x3OoWrPWn8S"
      },
      "source": [
        "Bemærk, at `np.arange` genererer en sekvens, der inkluderer den nedre grænse (5), men ikke den øvre grænse (12).\n",
        "Altså [5,12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiqqxDBINAOY"
      },
      "source": [
        "## Udfyld arrays med tilfældige tal\n",
        "\n",
        "NumPy giver forskellige funktioner til at udfylde arrays med tilfældige tal inden for visse intervaller. For eksempel genererer `np.random.randint` tilfældige heltal mellem en lav og en høj værdi. Følgende kald udfylder et array med 6 elementer med tilfældige heltal mellem 50 og 100.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tG8ao9CsNqw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5964845-2ef2-4f40-cba0-f65a5654d521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77 59 55 76 57 71]\n"
          ]
        }
      ],
      "source": [
        "random_integers_between_50_and_100 = np.random.randint(low=50, high=101, size=(6))\n",
        "print(random_integers_between_50_and_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSU7lMUcgRm3"
      },
      "source": [
        "Bemærk, at det højeste genererede heltal ved brug af `np.random.randint` er én mindre end argumentet `high`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQF6-Eg0ksqE"
      },
      "source": [
        "For at oprette tilfældige decimaltal mellem 0.0 og 1.0, kald `np.random.random`. For eksempel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Ny0eXZPk5Ax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88413ebe-16f6-4e1c-c8f1-697446612d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.16625408 0.78073314 0.42463751 0.1086769  0.65990799 0.64937478]\n"
          ]
        }
      ],
      "source": [
        "random_floats_between_0_and_1 = np.random.random([6])\n",
        "print(random_floats_between_0_and_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOdSjRlSEf6"
      },
      "source": [
        "## Matematiske operationer på NumPy-operand\n",
        "\n",
        "Hvis du ønsker at lægge to arrays sammen eller trække dem fra hinanden, kræver lineær algebra, at de to operander har de samme dimensioner. Derudover, hvis du ønsker at multiplicere to arrays, pålægger lineær algebra strenge regler for den dimensionelle kompatibilitet af operanderne. Heldigvis bruger NumPy en teknik kaldet [**broadcasting**](https://developers.google.com/machine-learning/glossary/#broadcasting) til at virtuelt udvide den mindre operand til dimensioner, der er kompatible med lineær algebra. For eksempel bruger følgende operation broadcasting til at tilføje 2.0 til værdien af hvert element i arrayet, der blev oprettet i den foregående kodecelle:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J5E5S0wjRvQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4340ad6e-683c-4624-c8b3-3fe66677e4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.16625408 2.78073314 2.42463751 2.1086769  2.65990799 2.64937478]\n"
          ]
        }
      ],
      "source": [
        "random_floats_between_2_and_3 = random_floats_between_0_and_1 + 2.0\n",
        "print(random_floats_between_2_and_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6K_poVDPpAg"
      },
      "source": [
        "Den følgende operation afhænger også af broadcasting for at multiplicere hvert element i et array med 3:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tYjvXmvFPoPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52eb7b45-89d7-41e4-f093-5a8ec3ceb025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[231 177 165 228 171 213]\n"
          ]
        }
      ],
      "source": [
        "random_integers_between_150_and_300 = random_integers_between_50_and_100 * 3\n",
        "print(random_integers_between_150_and_300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfYVa8iQTaUL"
      },
      "source": [
        "## Opgave 1: Opret et lineært datasæt\n",
        "\n",
        "Dit mål er at oprette et simpelt datasæt bestående af en enkelt funktion (feature) og en etiket (label) som følger:\n",
        "\n",
        "1. Tildel en sekvens af heltal fra 6 til 20 (inklusiv) til et NumPy-array kaldet `feature`.\n",
        "2. Tildel 15 værdier til et NumPy-array kaldet `label` på følgende måde:\n",
        "\n",
        "```\n",
        "   label = (3)(feature) + 4\n",
        "```\n",
        "For eksempel skal den første værdi for `label` være:\n",
        "\n",
        "```\n",
        "  label = (3)(6) + 4 = 22\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qK9UF2rUc3Y_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "f41d4b90-4714-429f-a945-00166002d41f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-cd0aaf9f220e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    feature = ? # write your code here\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "feature = ? # write your code here\n",
        "print(feature)\n",
        "label = ?   # write your code here\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KjtIAYvMTPGl"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to see a possible solution to Task 1.\n",
        "feature = np.arange(6, 21)\n",
        "print(feature)\n",
        "label = (feature * 3) + 4\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNsjGYRj87PB"
      },
      "source": [
        "## Opgave 2: Tilføj lidt støj til datasættet\n",
        "\n",
        "For at gøre dit datasæt lidt mere realistisk, skal du indsætte lidt tilfældig støj i hvert element i det `label`-array, du allerede har oprettet. Præcist sagt, skal du ændre hver værdi, der er tildelt `label`, ved at tilføje en *forskellig* tilfældig decimalværdi mellem -2 og +2.\n",
        "\n",
        "Stol ikke på broadcasting. Opret i stedet et `noise`-array med samme dimension som `label`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF-flFfs9r0q"
      },
      "outputs": [],
      "source": [
        "noise = ?    # write your code here\n",
        "print(noise)\n",
        "label = ?    # write your code here\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7chgYKrC93np"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to see a possible solution to Task 2.\n",
        "\n",
        "noise = (np.random.random([15]) * 4) - 2\n",
        "print(noise)\n",
        "label = label + noise\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeKRcpe49VnV"
      },
      "source": [
        "# Introduktion til Pandas DataFrame\n",
        "\n",
        "Her introducerer vi [**DataFrames**](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html), som er den centrale datastruktur i pandas API.\n",
        "\n",
        "En DataFrame ligner en hukommelsesskabelon. Ligesom en skabelon:\n",
        "\n",
        "  * En DataFrame opbevarer data i celler.\n",
        "  * En DataFrame har navngivne kolonner (normalt) og nummererede rækker.\n",
        "\n",
        "Som med NumPy kommer vi ikke til at gå i dybden med Pandas, men vi skal have en grundlæggende forståelse for at kunne bruge det til resten af vores 10 dages ML kursus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AByfHsr8H_sU"
      },
      "source": [
        "## Importér NumPy- og pandas-moduler\n",
        "\n",
        "Kør følgende kode for at importere NumPy- og pandas-modulerne.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmL0l551Iibq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RutIK84wIp1S"
      },
      "source": [
        "## Oprettelse af en DataFrame\n",
        "\n",
        "Den følgende kodecelle opretter en simpel DataFrame, der indeholder 10 celler, organiseret som følger:\n",
        "\n",
        "  * 5 rækker\n",
        "  * 2 kolonner, en kaldet `temperatur` og den anden kaldet `aktivitet`\n",
        "\n",
        "Den følgende kodecelle instantierer en `pd.DataFrame`-klasse for at generere en DataFrame. Klassen tager to argumenter:\n",
        "\n",
        "  * Det første argument giver dataene til at udfylde de 10 celler. Kodecellen kalder `np.array` for at generere en 5x2 NumPy-array.\n",
        "  * Det andet argument identificerer navnene på de to kolonner.\n",
        "\n",
        "**Bemærk**: Redefiner ikke variabler i den følgende kodecelle. Efterfølgende kodeceller bruger disse variabler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNZsPOgSD4F2"
      },
      "source": [
        "# Create and populate a 5x2 NumPy array.\n",
        "my_data = np.array([[0, 3], [10, 7], [20, 9], [30, 14], [40, 15]])\n",
        "\n",
        "# Create a Python list that holds the names of the two columns.\n",
        "my_column_names = ['temperature', 'activity']\n",
        "\n",
        "# Create a DataFrame.\n",
        "my_dataframe = pd.DataFrame(data=my_data, columns=my_column_names)\n",
        "\n",
        "# Print the entire DataFrame\n",
        "print(my_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ-I78_7OFVs"
      },
      "source": [
        "## Tilføjelse af en ny kolonne til en DataFrame\n",
        "\n",
        "Du kan tilføje en ny kolonne til en eksisterende pandas DataFrame ved blot at tildele værdier til et nyt kolonnenavn. For eksempel opretter den følgende kode en tredje kolonne kaldet `tilpasset` i `min_dataframe`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEBZyMdEOngx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4caccc-0b2a-4c66-e026-2cb0292a59bc"
      },
      "source": [
        "# Create a new column named adjusted.\n",
        "my_dataframe[\"adjusted\"] = my_dataframe[\"activity\"] + 2\n",
        "\n",
        "# Print the entire DataFrame\n",
        "print(my_dataframe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   temperature  activity  adjusted\n",
            "0            0         3         5\n",
            "1           10         7         9\n",
            "2           20         9        11\n",
            "3           30        14        16\n",
            "4           40        15        17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ2aziCR5th2"
      },
      "source": [
        "## Specificering af et udsnit af en DataFrame\n",
        "\n",
        "Pandas giver flere måder at isolere specifikke rækker, kolonner, slices eller celler i en DataFrame på."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIO91Fu65s6k"
      },
      "source": [
        "print(\"Rows #0, #1, and #2:\")\n",
        "print(my_dataframe.head(3), '\\n')\n",
        "\n",
        "print(\"Row #2:\")\n",
        "print(my_dataframe.iloc[[2]], '\\n')\n",
        "\n",
        "print(\"Rows #1, #2, and #3:\")\n",
        "print(my_dataframe[1:4], '\\n')\n",
        "\n",
        "print(\"Column 'temperature':\")\n",
        "print(my_dataframe['temperature'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cL_NxAdZzdS"
      },
      "source": [
        "## Opgave 1: Opret en DataFrame\n",
        "\n",
        "Gør følgende:\n",
        "\n",
        "1. Opret en 3x4 (3 rækker x 4 kolonner) pandas DataFrame, hvor kolonnerne har navnene `Eleanor`, `Chidi`, `Tahani` og `Jason`. Udfyld hver af de 12 celler i DataFrame med et tilfældigt heltal mellem 0 og 100.\n",
        "\n",
        "2. Output følgende:\n",
        "\n",
        "   * hele DataFrame\n",
        "   * værdien i cellen i række #1 i kolonnen `Eleanor`\n",
        "\n",
        "3. Opret en femte kolonne kaldet `Janet`, som udfyldes med række-for-række-summen af `Tahani` og `Jason`.\n",
        "\n",
        "I skal bruge kode fra den tidligere del omkring NumPy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIJEv08DMSxj"
      },
      "source": [
        "# Write your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPmpVM_8IoBO",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a955862-cb02-491e-dded-b827014f210d"
      },
      "source": [
        "#@title Double-click for a solution to Task 1.\n",
        "\n",
        "# Create a Python list that holds the names of the four columns.\n",
        "my_column_names = ['Eleanor', 'Chidi', 'Tahani', 'Jason']\n",
        "\n",
        "# Create a 3x4 numpy array, each cell populated with a random integer.\n",
        "my_data = np.random.randint(low=0, high=101, size=(3, 4))\n",
        "\n",
        "# Create a DataFrame.\n",
        "df = pd.DataFrame(data=my_data, columns=my_column_names)\n",
        "\n",
        "# Print the entire DataFrame\n",
        "print(df)\n",
        "\n",
        "# Print the value in row #1 of the Eleanor column.\n",
        "print(\"\\nSecond row of the Eleanor column: %d\\n\" % df['Eleanor'][1])\n",
        "\n",
        "# Create a column named Janet whose contents are the sum\n",
        "# of two other columns.\n",
        "df['Janet'] = df['Tahani'] + df['Jason']\n",
        "\n",
        "# Print the enhanced DataFrame\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Eleanor  Chidi  Tahani  Jason\n",
            "0        2     98      87     25\n",
            "1       39      8      77     80\n",
            "2        7     43      18     86\n",
            "\n",
            "Second row of the Eleanor column: 39\n",
            "\n",
            "   Eleanor  Chidi  Tahani  Jason  Janet\n",
            "0        2     98      87     25    112\n",
            "1       39      8      77     80    157\n",
            "2        7     43      18     86    104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh7MeyafemNL"
      },
      "source": [
        "## Kopiering af en DataFrame\n",
        "Pandas giver to forskellige måder at kopiere en DataFrame på:\n",
        "\n",
        "* **Reference.** Hvis du tildeler en DataFrame til en ny variabel, vil eventuelle ændringer i DataFrame eller den nye variabel afspejles i den anden.\n",
        "* **Kopiering.** Hvis du kalder metoden `pd.DataFrame.copy`, opretter du en ægte uafhængig kopi. Ændringer i den originale DataFrame eller kopien vil ikke blive afspejlet i den anden.\n",
        "\n",
        "Forskellen er subtil, men vigtig.\n",
        "I kender det nok fra at have arbejde med arrays i andre sprog, hvor der er forskel på om det peger på det samme sted i memory eller om den laver en helt ny kopi!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDu2VotPgzsW"
      },
      "source": [
        "# Create a reference by assigning my_dataframe to a new variable.\n",
        "print(\"Experiment with a reference:\")\n",
        "reference_to_df = df\n",
        "\n",
        "# Print the starting value of a particular cell.\n",
        "print(\"  Starting value of df: %d\" % df['Jason'][1])\n",
        "print(\"  Starting value of reference_to_df: %d\\n\" % reference_to_df['Jason'][1])\n",
        "\n",
        "# Modify a cell in df.\n",
        "df.at[1, 'Jason'] = df['Jason'][1] + 5\n",
        "print(\"  Updated df: %d\" % df['Jason'][1])\n",
        "print(\"  Updated reference_to_df: %d\\n\\n\" % reference_to_df['Jason'][1])\n",
        "\n",
        "# Create a true copy of my_dataframe\n",
        "print(\"Experiment with a true copy:\")\n",
        "copy_of_my_dataframe = my_dataframe.copy()\n",
        "\n",
        "# Print the starting value of a particular cell.\n",
        "print(\"  Starting value of my_dataframe: %d\" % my_dataframe['activity'][1])\n",
        "print(\"  Starting value of copy_of_my_dataframe: %d\\n\" % copy_of_my_dataframe['activity'][1])\n",
        "\n",
        "# Modify a cell in df.\n",
        "my_dataframe.at[1, 'activity'] = my_dataframe['activity'][1] + 3\n",
        "print(\"  Updated my_dataframe: %d\" % my_dataframe['activity'][1])\n",
        "print(\"  copy_of_my_dataframe does not get updated: %d\" % copy_of_my_dataframe['activity'][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "m4_xVI2ibhmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teori som bliver gennemgået på klassen - [Notion Regression](https://www.notion.so/mercantec/Machine-Learning-e89a2baf0d414172b13d07465366482e?pvs=4#cceeafe8d5b9432d8709b1329caf6969)"
      ],
      "metadata": {
        "id": "TMh10j4cclyk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "## Simpel lineær regression med syntetiske data\n",
        "\n",
        "I denne første del vil vi udforske lineær regression med en simpel database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZz29MwGgE2Y"
      },
      "source": [
        "### Læringsmål:\n",
        "\n",
        "Efter at have udført denne øvelse vil du vide, hvordan du gør følgende:\n",
        "\n",
        "  * Tilpas følgende [hyperparametre](https://developers.google.com/machine-learning/glossary/#hyperparameter):\n",
        "    * [læringsrate](https://developers.google.com/machine-learning/glossary/#learning_rate)\n",
        "    * antal [epoker](https://developers.google.com/machine-learning/glossary/#epoch)\n",
        "    * [batchstørrelse](https://developers.google.com/machine-learning/glossary/#batch_size)\n",
        "  * Tolker forskellige typer af [loss curves](https://developers.google.com/machine-learning/glossary/#loss_curve)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchnxAsaKKqO"
      },
      "source": [
        "### Importér relevante moduler\n",
        "\n",
        "Den følgende celle importerer de pakker, som programmet kræver:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIpsyJITPcbG"
      },
      "source": [
        "## Definér funktioner, der opbygger og træner en model\n",
        "\n",
        "Den følgende kode definerer to funktioner:\n",
        "\n",
        "  * `build_model(my_learning_rate)`, som opbygger en tom model.\n",
        "  * `train_model(model, feature, label, epochs)`, som træner modellen ud fra de eksempler (feature og label), du passerer.\n",
        "\n",
        "Da du ikke behøver at forstå modelopbygningskoden lige nu, har vi skjult denne kodecelle. Du kan valgfrit dobbeltklikke på overskriften for at udforske denne kode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xvO_beKVP1Ke",
        "outputId": "7a193f43-e0f5-4864-9544-9e575ba84543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined build_model and train_model\n"
          ]
        }
      ],
      "source": [
        "#@title Define the functions that build and train a model\n",
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  # A sequential model contains one or more layers.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,\n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that\n",
        "  # TensorFlow can efficiently execute. Configure\n",
        "  # training to minimize the model's mean squared error.\n",
        "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the feature values and the label values to the\n",
        "  # model. The model will train for the specified number\n",
        "  # of epochs, gradually learning how the feature values\n",
        "  # relate to the label values.\n",
        "  history = model.fit(x=feature,\n",
        "                      y=label,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the\n",
        "  # rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Gather the history (a snapshot) of each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # Specifically gather the model's root mean\n",
        "  # squared error at each epoch.\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse\n",
        "\n",
        "print(\"Defined build_model and train_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "### Definér plotningsfunktioner\n",
        "\n",
        "Vi bruger en populær Python-bibliotek kaldet [Matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) til at oprette følgende to plot:\n",
        "\n",
        "* et plot af funktionens værdier vs. etiketværdierne og en linje, der viser output af den trænede model.\n",
        "* en [tabkurve](https://developers.google.com/machine-learning/glossary/#loss_curve).\n",
        "\n",
        "Vi har skjult den følgende kodecelle, fordi det ikke er relevant at lære Matplotlib i forhold til læringsmålene. Uanset det skal du stadig køre alle skjulte kodeceller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QF0BFRXTOeR3",
        "outputId": "0785b5b3-e5a2-4383-a1c2-d5d809c936ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined the plot_the_model and plot_the_loss_curve functions.\n"
          ]
        }
      ],
      "source": [
        "#@title Define the plotting functions\n",
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel(\"feature\")\n",
        "  plt.ylabel(\"label\")\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "  plt.scatter(feature, label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = feature[-1]\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "  plt.show()\n",
        "\n",
        "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Eg08C7ZdxaMe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVSDPusELEZ5"
      },
      "source": [
        "### Definér datasættet\n",
        "\n",
        "Datasættet består af 12 [eksempler](https://developers.google.com/machine-learning/glossary/#example). Hvert eksempel består af en [feature](https://developers.google.com/machine-learning/glossary/#feature) og en [label](https://developers.google.com/machine-learning/glossary/#label).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnUSYKw4LUuh"
      },
      "outputs": [],
      "source": [
        "my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
        "my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K24afla-4s2x"
      },
      "source": [
        "### Angiv hyperparametrene\n",
        "\n",
        "Hyperparametrene i denne Colab er som følger:\n",
        "\n",
        "  * [læringsrate - learning_rate](https://developers.google.com/machine-learning/glossary/#learning_rate)\n",
        "  * [epochs](https://developers.google.com/machine-learning/glossary/#epoch)\n",
        "  * [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size)\n",
        "\n",
        "Den følgende kodecelle initialiserer disse hyperparametre og kalder derefter funktionerne, der opbygger og træner modellen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye730h13CQ97"
      },
      "outputs": [],
      "source": [
        "learning_rate=0.01\n",
        "epochs=10\n",
        "my_batch_size=12\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwSm60H6pQjJ"
      },
      "source": [
        "### Opgave 1: Undersøg graferne\n",
        "\n",
        "Undersøg den øverste graf. De blå prikker identificerer de faktiske data, mens den røde linje identificerer output fra den trænede model. Ideelt set bør den røde linje justere sig pænt med de blå prikker. Gør den det?\n",
        "\n",
        "En vis grad af tilfældighed spiller ind i træningen af en model, så du vil få lidt forskellige resultater hver gang du træner. Det sagt, medmindre du er en ekstremt heldig person, så passer den røde linje sandsynligvis *ikke* pænt med de blå prikker.\n",
        "\n",
        "Undersøg den nederste graf, som viser tabkurven. Bemærk, at tabkurven falder, men ikke flader ud, hvilket er et tegn på, at modellen ikke er tilstrækkeligt trænet.\n",
        "\n",
        "Hvad kan have effekt på hvor godt modellen er trænet?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLXPvqCRvgI4"
      },
      "source": [
        "### Opgave 2: Øg antallet af epoker\n",
        "\n",
        "Træningstabet bør gradvist falde, først stejlt og derefter langsommere. Til sidst bør træningstab forblive stabilt (nul hældning eller næsten nul hældning), hvilket indikerer, at træningen har [konvergeret](http://developers.google.com/machine-learning/glossary/#convergence).\n",
        "\n",
        "I Opgave 1 konvergerede træningstab ikke. En mulig løsning er at øge antallet af epoker tilstrækkeligt for at få modellen til at konvergere. Det er imidlertid ineffektivt at træne ud over konvergenspunktet, så indstil ikke bare antallet af epoker til en vilkårligt høj værdi.\n",
        "\n",
        "Undersøg tabkurven. Konvergerer modellen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXuJH3h6t5qs"
      },
      "outputs": [],
      "source": [
        "learning_rate=0.01\n",
        "epochs= ?   # Replace ? with an integer.\n",
        "my_batch_size=12\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                        my_label, epochs,\n",
        "                                                        my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tWrzP4Ww7sD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view a possible solution\n",
        "learning_rate=0.01\n",
        "epochs=450\n",
        "my_batch_size=12\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "# The loss curve suggests that the model does converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KmzfFB5zwvd"
      },
      "source": [
        "### Opgave 3: Forøg læringsraten\n",
        "\n",
        "I Opgave 2 øgede du antallet af epoker for at få modellen til at konvergere. Nogle gange kan du få modellen til at konvergere hurtigere ved at øge læringsraten. Dog gør indstillingen af læringsraten for høj ofte det umuligt for en model at konvergere. I Opgave 3 har vi med vilje indstillet læringsraten for høj. Kør følgende kodecelle og se, hvad der sker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD1hTmdd0uCo"
      },
      "outputs": [],
      "source": [
        "# Increase the learning rate and decrease the number of epochs.\n",
        "learning_rate=100\n",
        "epochs=500\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c96ITm021NEV"
      },
      "source": [
        "Den resulterende model er elendig; den røde linje justerer sig ikke med de blå prikker. Desuden svinger tabkurven som en [rutsjebane](https://www.wikipedia.org/wiki/Roller_coaster). En svingende tabkurve tyder kraftigt på, at læringsraten er for høj.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63YkMx82WVr"
      },
      "source": [
        "### Opgave 4: Find den ideelle kombination af epoker og læringsrate\n",
        "\n",
        "Tildel værdier til følgende to hyperparametre for at få træningen til at konvergere så effektivt som muligt:\n",
        "\n",
        "* Læringsrate (learning_rate)\n",
        "* Epoker (epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYC8eR5x5n4m"
      },
      "outputs": [],
      "source": [
        "# Set the learning rate and number of epochs\n",
        "learning_rate= ?  # Replace ? with a floating-point number\n",
        "epochs= ?   # Replace ? with an integer\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_GMGgR6O54IN"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view a possible solution\n",
        "\n",
        "learning_rate=0.14\n",
        "epochs=70\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDET9e6AAbA"
      },
      "source": [
        "### Opgave 5: Justér batchstørrelsen\n",
        "\n",
        "Systemet genberegner modelens tabværdi og justerer modelens vægte og bias efter hver **iteration**. Hver iteration er det interval, hvor systemet behandler én batch. For eksempel, hvis **batchstørrelsen** er 6, genberegner systemet modelens tabværdi og justerer modelens vægte og bias efter at have behandlet hver 6 eksempler.\n",
        "\n",
        "Én **epoke** dækker tilstrækkeligt mange iterationer til at behandle hver eneste eksempel i datasættet. For eksempel, hvis batchstørrelsen er 12, varer hver epoke én iteration. Men hvis batchstørrelsen er 6, kræver hver epoke to iterationer.\n",
        "\n",
        "Det er fristende blot at indstille batchstørrelsen til antallet af eksempler i datasættet (12 i dette tilfælde). Dog kan modellen faktisk trænes hurtigere på mindre batches. Omvendt indeholder meget små batches måske ikke nok information til at hjælpe modellen med at konvergere.\n",
        "\n",
        "Experimentér med `my_batch_size` i den følgende kodecelle. Hvad er det mindste heltal, du kan indstille for `my_batch_size` og stadig få modellen til at konvergere på hundrede epoker?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vGx0lOodQrT"
      },
      "outputs": [],
      "source": [
        "learning_rate=0.05\n",
        "epochs=100\n",
        "my_batch_size= ?  # Replace ? with an integer.\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                        my_label, epochs,\n",
        "                                                        my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mtVpoBrANAm"
      },
      "outputs": [],
      "source": [
        "#@title Double-click to view a possible solution\n",
        "\n",
        "learning_rate=0.05\n",
        "epochs=100\n",
        "my_batch_size=1 # Wow, a batch size of 1 works!\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,\n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS3q7TIF9SFL"
      },
      "source": [
        "## Sammenfatning af hyperparameter-tilpasning\n",
        "\n",
        "De fleste maskinlæringsproblemer kræver en masse hyperparameter-tilpasning. Desværre kan vi ikke give konkrete retningslinjer for tilpasning til hver model. At sænke læringsraten kan hjælpe én model med at konvergere effektivt, men få en anden model til at konvergere alt for langsomt. Du skal eksperimentere for at finde den bedste sæt hyperparametre for dit datasæt. Med det sagt er her nogle tommelfingerregler:\n",
        "\n",
        "* Træningstabet bør jævnt falde, først stejlt og derefter langsommere, indtil kurvens hældning når eller nærmer sig nul.\n",
        "* Hvis træningstabet ikke konvergerer, så træn i flere epoker.\n",
        "* Hvis træningstabet falder for langsomt, skal du øge læringsraten. Bemærk, at indstilling af læringsraten for høj også kan forhindre træningstab i at konvergere.\n",
        "* Hvis træningstabet varierer vildt (dvs. træningstabet hopper rundt), så sænk læringsraten.\n",
        "* At sænke læringsraten samtidig med at du øger antallet af epoker eller batchstørrelsen er ofte en god kombination.\n",
        "* At indstille batchstørrelsen til et *meget* lille batchnummer kan også forårsage ustabilitet. Prøv først store batchstørrelsesværdier. Derefter skal du sænke batchstørrelsen, indtil du ser forringelse.\n",
        "* For datasæt fra den virkelige verden, der består af et meget stort antal eksempler, kan hele datasættet muligvis ikke passe i hukommelsen. I sådanne tilfælde skal du reducere batchstørrelsen for at muliggøre, at en batch passer i hukommelsen.\n",
        "\n",
        "Husk: den ideelle kombination af hyperparametre afhænger af data, så du skal altid eksperimentere og verificere.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_yvxKvCbU7U"
      },
      "source": [
        "## Linear Regression with a Real Dataset\n",
        "   \n",
        "Nu prøver vi at skifter over til et reelt datasæt med huspriser i Californien.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wtceyJj2uX"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "After doing this Colab, you'll know how to do the following:\n",
        "\n",
        "  * Read a .csv file into a [pandas](https://developers.google.com/machine-learning/glossary/#pandas) DataFrame.\n",
        "  * Examine a [dataset](https://developers.google.com/machine-learning/glossary/#data_set).\n",
        "  * Experiment with different [features](https://developers.google.com/machine-learning/glossary/#feature) in building a model.\n",
        "  * Tune the model's [hyperparameters](https://developers.google.com/machine-learning/glossary/#hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZEgJQSjyK4"
      },
      "source": [
        "### The Dataset\n",
        "  \n",
        "The [dataset for this exercise](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) is based on 1990 census data from California. The dataset is old but still provides a great opportunity to learn about machine learning programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_Io78sbbU7V"
      },
      "source": [
        "### Import relevant modules\n",
        "\n",
        "The following hidden code cell imports the necessary code to run the code in the rest of this Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LdzASjKFbU7V"
      },
      "source": [
        "#@title Import relevant modules\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "### The dataset\n",
        "\n",
        "Datasets are often stored on disk or at a URL in [.csv format](https://wikipedia.org/wiki/Comma-separated_values).\n",
        "\n",
        "A well-formed .csv file contains column names in the first row, followed by many rows of data.  A comma divides each value in each row. For example, here are the first five rows of the .csv file holding the California Housing Dataset:\n",
        "\n",
        "```\n",
        "\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n",
        "-114.310000,34.190000,15.000000,5612.000000,1283.000000,1015.000000,472.000000,1.493600,66900.000000\n",
        "-114.470000,34.400000,19.000000,7650.000000,1901.000000,1129.000000,463.000000,1.820000,80100.000000\n",
        "-114.560000,33.690000,17.000000,720.000000,174.000000,333.000000,117.000000,1.650900,85700.000000\n",
        "-114.570000,33.640000,14.000000,1501.000000,337.000000,515.000000,226.000000,3.191700,73400.000000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSFQkzNlj-l6"
      },
      "source": [
        "### Load the .csv file into a pandas DataFrame\n",
        "\n",
        "This Colab, like many machine learning programs, gathers the .csv file and stores the data in memory as a pandas Dataframe.  Pandas is an open source Python library.  The primary datatype in pandas is a DataFrame.  You can imagine a pandas DataFrame as a spreadsheet in which each row is identified by a number and each column by a name. Pandas is itself built on another open source Python library called NumPy. If you aren't familiar with these technologies, please view these two quick tutorials:\n",
        "\n",
        "*   [NumPy](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=numpy_tf2-colab&hl=en)\n",
        "*   [Pandas DataFrames](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=pandas_tf2-colab&hl=en)\n",
        "\n",
        "The following code cell imports the .csv file into a pandas DataFrame and scales the values in the label (`median_house_value`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "# Import the dataset.\n",
        "training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "# Scale the label.\n",
        "training_df[\"median_house_value\"] /= 1000.0\n",
        "\n",
        "# Print the first rows of the pandas DataFrame.\n",
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5inxx49n4U9u"
      },
      "source": [
        "Scaling `median_house_value` puts the value of each house in units of thousands. Scaling will keep loss values and learning rates in a friendlier range.  \n",
        "\n",
        "Although scaling a label is usually *not* essential, scaling features in a multi-feature model usually *is* essential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMysi6-3IAbu"
      },
      "source": [
        "### Examine the dataset\n",
        "\n",
        "A large part of most machine learning projects is getting to know your data. The pandas API provides a `describe` function that outputs the following statistics about every column in the DataFrame:\n",
        "\n",
        "* `count`, which is the number of rows in that column. Ideally, `count` contains the same value for every column.\n",
        "\n",
        "* `mean` and `std`, which contain the mean and standard deviation of the values in each column.\n",
        "\n",
        "* `min` and `max`, which contain the lowest and highest values in each column.\n",
        "\n",
        "* `25%`, `50%`, `75%`, which contain various [quantiles](https://developers.google.com/machine-learning/glossary/#quantile)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_vFy_6ZbU7W"
      },
      "source": [
        "# Get statistics on the dataset.\n",
        "training_df.describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9pcW_Yjtoo8"
      },
      "source": [
        "### Task 1: Identify anomalies in the dataset\n",
        "\n",
        "Do you see any anomalies (strange values) in the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoS7NWRXEs1H",
        "cellView": "form"
      },
      "source": [
        "#@title Double-click to view a possible answer.\n",
        "\n",
        "# The maximum value (max) of several columns seems very\n",
        "# high compared to the other quantiles. For example,\n",
        "# example the total_rooms column. Given the quantile\n",
        "# values (25%, 50%, and 75%), you might expect the\n",
        "# max value of total_rooms to be approximately\n",
        "# 5,000 or possibly 10,000. However, the max value\n",
        "# is actually 37,937.\n",
        "\n",
        "# When you see anomalies in a column, become more careful\n",
        "# about using that column as a feature. That said,\n",
        "# anomalies in potential features sometimes mirror\n",
        "# anomalies in the label, which could make the column\n",
        "# be (or seem to be) a powerful feature.\n",
        "# Also, as you will see later in the course, you\n",
        "# might be able to represent (pre-process) raw data\n",
        "# in order to make columns into useful features."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "### Define functions that build and train a model\n",
        "\n",
        "The following code defines two functions:\n",
        "\n",
        "  * `build_model(my_learning_rate)`, which builds a randomly-initialized model.\n",
        "  * `train_model(model, feature, label, epochs)`, which trains the model from the examples (feature and label) you pass.\n",
        "\n",
        "Since you don't need to understand model building code right now, we've hidden this code cell.  You may optionally double-click the following headline to see the code that builds and trains a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pedD5GhlDC-y",
        "cellView": "form"
      },
      "source": [
        "#@title Define the functions that build and train a model\n",
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,\n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that TensorFlow can efficiently\n",
        "  # execute. Configure training to minimize the model's mean squared error.\n",
        "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, df, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the model the feature and the label.\n",
        "  # The model will train for the specified number of epochs.\n",
        "  history = model.fit(x=df[feature],\n",
        "                      y=df[label],\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # To track the progression of training, we're going to take a snapshot\n",
        "  # of the model's root mean squared error at each epoch.\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse\n",
        "\n",
        "print(\"Defined the build_model and train_model functions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndnSl7yTbU7W"
      },
      "source": [
        "### Define plotting functions\n",
        "\n",
        "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) functions create the following plots:\n",
        "\n",
        "*  a scatter plot of the feature vs. the label, and a line showing the output of the trained model\n",
        "*  a loss curve\n",
        "\n",
        "You may optionally double-click the headline to see the matplotlib code, but note that writing matplotlib code is not an important part of learning ML programming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VZN7O23xbU7W"
      },
      "source": [
        "#@title Define the plotting functions\n",
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel(feature)\n",
        "  plt.ylabel(label)\n",
        "\n",
        "  # Create a scatter plot from 200 random points of the dataset.\n",
        "  random_examples = training_df.sample(n=200)\n",
        "  plt.scatter(random_examples[feature], random_examples[label])\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = random_examples[feature].max()\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "  plt.show()\n",
        "\n",
        "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "### Call the model functions\n",
        "\n",
        "An important part of machine learning is determining which [features](https://developers.google.com/machine-learning/glossary/#feature) correlate with the [label](https://developers.google.com/machine-learning/glossary/#label). For example, real-life home-value prediction models typically rely on hundreds of features and synthetic features. However, this model relies on only one feature. For now, you'll arbitrarily use `total_rooms` as that feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj3v5EKQFY8s",
        "cellView": "both"
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 30\n",
        "batch_size = 30\n",
        "\n",
        "# Specify the feature and the label.\n",
        "my_feature = \"total_rooms\"  # the total number of rooms on a specific city block.\n",
        "my_label=\"median_house_value\" # the median value of a house on a specific city block.\n",
        "# That is, you're going to create a model that predicts house value based\n",
        "# solely on total_rooms.\n",
        "\n",
        "# Discard any pre-existing version of the model.\n",
        "my_model = None\n",
        "\n",
        "# Invoke the functions.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
        "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
        "\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btp8zUNbYOcd"
      },
      "source": [
        "A certain amount of randomness plays into training a model. Consequently, you'll get different results each time you train the model. That said, given the dataset and the hyperparameters, the trained model will generally do a poor job describing the feature's relation to the label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xNqWWos_zyk"
      },
      "source": [
        "### Use the model to make predictions\n",
        "\n",
        "You can use the trained model to make predictions. In practice, [you should make predictions on examples that are not used in training](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). However, for this exercise, you'll just work with a subset of the same training dataset. A later Colab exercise will explore ways to make predictions on examples not used in training.\n",
        "\n",
        "First, run the following code to define the house prediction function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH63BmncAcab"
      },
      "source": [
        "def predict_house_values(n, feature, label):\n",
        "  \"\"\"Predict house values based on a feature.\"\"\"\n",
        "\n",
        "  batch = training_df[feature][10000:10000 + n]\n",
        "  predicted_values = my_model.predict_on_batch(x=batch)\n",
        "\n",
        "  print(\"feature   label          predicted\")\n",
        "  print(\"  value   value          value\")\n",
        "  print(\"          in thousand$   in thousand$\")\n",
        "  print(\"--------------------------------------\")\n",
        "  for i in range(n):\n",
        "    print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000 + i],\n",
        "                                   training_df[label][10000 + i],\n",
        "                                   predicted_values[i][0] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBNQujU5WjK"
      },
      "source": [
        "Now, invoke the house prediction function on 10 examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_0DGBt0Kz_N"
      },
      "source": [
        "predict_house_values(10, my_feature, my_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGaqArcpqY3"
      },
      "source": [
        "### Task 2: Judge the predictive power of the model\n",
        "\n",
        "Look at the preceding table. How close is the predicted value to the label value?  In other words, does your model accurately predict house values?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVpjhUFm9uID",
        "cellView": "form"
      },
      "source": [
        "#@title Double-click to view the answer.\n",
        "\n",
        "# Most of the predicted values differ significantly\n",
        "# from the label value, so the trained model probably\n",
        "# doesn't have much predictive power. However, the\n",
        "# first 10 examples might not be representative of\n",
        "# the rest of the examples."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLoqis3IUPSd"
      },
      "source": [
        "### Task 3: Try a different feature\n",
        "\n",
        "The `total_rooms` feature had only a little predictive power. Would a different feature have greater predictive power?  Try using `population` as the feature instead of `total_rooms`.\n",
        "\n",
        "Note: When you change features, you might also need to change the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ab6HD4ZO75"
      },
      "source": [
        "my_feature = \"?\"   # Replace the ? with population or possibly\n",
        "                   # a different column name.\n",
        "\n",
        "# Experiment with the hyperparameters.\n",
        "learning_rate = 2\n",
        "epochs = 3\n",
        "batch_size = 120\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(15, my_feature, my_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "107mDkW7U6mg",
        "cellView": "form"
      },
      "source": [
        "#@title Double-click to view a possible solution.\n",
        "\n",
        "my_feature = \"population\" # Pick a feature other than \"total_rooms\"\n",
        "\n",
        "# Possibly, experiment with the hyperparameters.\n",
        "learning_rate = 0.05\n",
        "epochs = 18\n",
        "batch_size = 3\n",
        "\n",
        "# Don't change anything below.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(10, my_feature, my_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd_rHJ59AUtk"
      },
      "source": [
        "Did `population` produce better predictions than `total_rooms`?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0tPEtzcC-vK",
        "cellView": "form"
      },
      "source": [
        "#@title Double-click to view the answer.\n",
        "\n",
        "# Training is not entirely deterministic, but population\n",
        "# typically converges at a slightly higher RMSE than\n",
        "# total_rooms.  So, population appears to be about\n",
        "# the same or slightly worse at making predictions\n",
        "# than total_rooms."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uYpyGacsIg"
      },
      "source": [
        "### Task 4: Define a synthetic feature\n",
        "\n",
        "You have determined that `total_rooms` and `population` were not useful features.  That is, neither the total number of rooms in a neighborhood nor the neighborhood's population successfully predicted the median house price of that neighborhood. Perhaps though, the *ratio* of `total_rooms` to `population` might have some predictive power. That is, perhaps block density relates to median house value.\n",
        "\n",
        "To explore this hypothesis, do the following:\n",
        "\n",
        "1. Create a [synthetic feature](https://developers.google.com/machine-learning/glossary/#synthetic_feature) that's a ratio of `total_rooms` to `population`. (If you are new to pandas DataFrames, please study the [Pandas DataFrame Ultraquick Tutorial](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=pandas_tf2-colab&hl=en).)\n",
        "2. Tune the three hyperparameters.\n",
        "3. Determine whether this synthetic feature produces\n",
        "   a lower loss value than any of the single features you\n",
        "   tried earlier in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kx2xHSgdcpg"
      },
      "source": [
        "# Define a synthetic feature named rooms_per_person\n",
        "training_df[\"rooms_per_person\"] = ? # write your code here.\n",
        "\n",
        "# Don't change the next line.\n",
        "my_feature = \"rooms_per_person\"\n",
        "\n",
        "# Assign values to these three hyperparameters.\n",
        "learning_rate = ?\n",
        "epochs = ?\n",
        "batch_size = ?\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "predict_house_values(15, my_feature, my_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRfxp_3yofe3",
        "cellView": "form"
      },
      "source": [
        "#@title Double-click to view a possible solution to Task 4.\n",
        "\n",
        "# Define a synthetic feature\n",
        "training_df[\"rooms_per_person\"] = training_df[\"total_rooms\"] / training_df[\"population\"]\n",
        "my_feature = \"rooms_per_person\"\n",
        "\n",
        "# Tune the hyperparameters.\n",
        "learning_rate = 0.06\n",
        "epochs = 24\n",
        "batch_size = 30\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, mae = train_model(my_model, training_df,\n",
        "                                        my_feature, my_label,\n",
        "                                        epochs, batch_size)\n",
        "\n",
        "plot_the_loss_curve(epochs, mae)\n",
        "predict_house_values(15, my_feature, my_label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBiDWursB1Wi"
      },
      "source": [
        "Based on the loss values, this synthetic feature produces a better model than the individual features you tried in Task 2 and Task 3. However, the model still isn't creating great predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEG_9oU9O54u"
      },
      "source": [
        "### Task 5. Find feature(s) whose raw values correlate with the label\n",
        "\n",
        "So far, we've relied on trial-and-error to identify possible features for the model.  Let's rely on statistics instead.\n",
        "\n",
        "A **correlation matrix** indicates how each attribute's raw values relate to the other attributes' raw values. Correlation values have the following meanings:\n",
        "\n",
        "  * `1.0`: perfect positive correlation; that is, when one attribute rises, the other attribute rises.\n",
        "  * `-1.0`: perfect negative correlation; that is, when one attribute rises, the other attribute falls.\n",
        "  * `0.0`: no correlation; the two columns [are not linearly related](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg).\n",
        "\n",
        "In general, the higher the absolute value of a correlation value, the greater its predictive power. For example, a correlation value of -0.8 implies far more predictive power than a correlation of -0.2.\n",
        "\n",
        "The following code cell generates the correlation matrix for attributes of the California Housing Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFGKL45LO8Tt"
      },
      "source": [
        "# Generate a correlation matrix.\n",
        "training_df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp0r3NAVPEdt"
      },
      "source": [
        "The correlation matrix shows nine potential features (including a synthetic\n",
        "feature) and one label (`median_house_value`).  A strong negative correlation or strong positive correlation with the label suggests a potentially good feature.  \n",
        "\n",
        "**Your Task:** Determine which of the nine potential features appears to be the best candidate for a feature?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RomQTd1OPVd0",
        "cellView": "form"
      },
      "source": [
        "#@title Double-click here for the solution to Task 5\n",
        "\n",
        "# The median_income correlates 0.7 with the label\n",
        "# (median_house_value), so median_income might be a\n",
        "# good feature. The other seven potential features\n",
        "# all have a correlation relatively close to 0.\n",
        "\n",
        "# If time permits, try median_income as the feature\n",
        "# and see whether the model improves."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RqvEbaVSlRt"
      },
      "source": [
        "Correlation matrices don't tell the entire story. In later exercises, you'll find additional ways to unlock predictive power from potential features.\n",
        "\n",
        "**Note:** Using `median_income` as a feature may raise some ethical and fairness\n",
        "issues. Towards the end of the course, we'll explore ethical and fairness issues."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Klassifikaiton"
      ],
      "metadata": {
        "id": "jY-QAVIhcDDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Teori** - [**Notion Klassifikation**](https://www.notion.so/mercantec/Machine-Learning-e89a2baf0d414172b13d07465366482e?pvs=4#1a2aef001e294c9bb2354f79b3db80d3)"
      ],
      "metadata": {
        "id": "tEo0HmdIcc20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "r843-s6ZcF8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teori - [Notion Clustering](https://www.notion.so/mercantec/Machine-Learning-e89a2baf0d414172b13d07465366482e?pvs=4#2c11e4d51b994deda5dcdbcbb5063c2c)"
      ],
      "metadata": {
        "id": "_3kTI9T4cR0s"
      }
    }
  ]
}